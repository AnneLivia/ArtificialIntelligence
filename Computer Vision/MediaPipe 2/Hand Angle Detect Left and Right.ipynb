{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of mediapipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Pipeline of the MediaPipe Hands: \n",
    "\n",
    "MediaPipe Hands is a high-fidelity hand and finger tracking solution. It employs machine learning (ML) to infer **21 3D landmarks** of a hand from just a single frame. Whereas current state-of-the-art approaches rely primarily on powerful desktop environments for inference, our method achieves real-time performance on a mobile phone, and even scales to multiple hands.\n",
    "\n",
    "*Tracked 3D hand landmarks are represented by dots in different shades, with the brighter ones denoting landmarks closer to the camera.*\n",
    "\n",
    "MediaPipe Hands utilizes an **ML pipeline** consisting of multiple models working together: A **palm detection model** that operates on the full image and returns an **oriented hand bounding box**. A **hand landmark model** that operates on the cropped image region defined by the palm detector and returns **high-fidelity 3D hand keypoints**. This strategy is similar to that employed in our MediaPipe Face Mesh solution, which uses a face detector together with a face landmark model.\n",
    "\n",
    "Providing the accurately cropped hand image to the hand landmark model drastically reduces the need for data augmentation (e.g. rotations, translation and scale) and instead allows the network to dedicate most of its capacity towards coordinate prediction accuracy. In addition, in our pipeline the crops can also be generated based on the hand landmarks identified in the previous frame, and only when the landmark model could no longer identify hand presence is palm detection invoked to relocalize the hand.\n",
    "\n",
    "### Hand Landmark Model:\n",
    "\n",
    "\n",
    "<img src=https://google.github.io/mediapipe/images/mobile/hand_landmarks.png />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp # is going to import mediapipe solutions\n",
    "import cv2 # opencv\n",
    "import numpy as np\n",
    "# those libraries bellow are mainly for output process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now open the webcam using opencv\n",
    "\n",
    "Supported configuration options:\n",
    "\n",
    "- static_image_mode\n",
    "- max_num_hands\n",
    "- model_complexity\n",
    "- min_detection_confidence\n",
    "- min_tracking_confidence\n",
    "\n",
    "\n",
    "STATIC_IMAGE_MODE\n",
    "If set to false, the solution treats the input images as a video stream. It will try to detect hands in the first input images, and upon a successful detection further localizes the hand landmarks. In subsequent images, once all max_num_hands hands are detected and the corresponding hand landmarks are localized, it simply tracks those landmarks without invoking another detection until it loses track of any of the hands. This reduces latency and is ideal for processing video frames. If set to true, hand detection runs on every input image, ideal for processing a batch of static, possibly unrelated, images. Default to false.\n",
    "\n",
    "MAX_NUM_HANDS\n",
    "Maximum number of hands to detect. Default to 2.\n",
    "\n",
    "MODEL_COMPLEXITY\n",
    "Complexity of the hand landmark model: 0 or 1. Landmark accuracy as well as inference latency generally go up with the model complexity. Default to 1.\n",
    "\n",
    "MIN_DETECTION_CONFIDENCE\n",
    "Minimum confidence value ([0.0, 1.0]) from the hand detection model for the detection to be considered successful. Default to 0.5.\n",
    "\n",
    "MIN_TRACKING_CONFIDENCE:\n",
    "Minimum confidence value ([0.0, 1.0]) from the landmark-tracking model for the hand landmarks to be considered tracked successfully, or otherwise hand detection will be invoked automatically on the next input image. Setting it to a higher value can increase robustness of the solution, at the expense of a higher latency. Ignored if static_image_mode is true, where hand detection simply runs on every image. Default to 0.5.\n",
    "\n",
    "Confidence - detection: threshold for the initial detection to be succesful\n",
    "\n",
    "Confidence - tracking: threshold for  tracking after initial detection\n",
    "\n",
    "you can use Pythonâ€™s **enumerate()** to get a counter and the value from the iterable at the same time!\n",
    "DrawingSpec is a mediapipe class that allows you to customize the look of your detection\n",
    "in this case we are using to customize the landmarks and connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_hands.Hands(min_detection_confidence=0.8, min_tracking_confidence=0.5) as hands:\n",
    "    # start the loop\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        frame = cv2.flip(frame, 1)\n",
    "        \n",
    "        # by default the image color is blue green and red, rgb, red green and blue\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # set flag\n",
    "        image.flags.writeable = False\n",
    "        \n",
    "        # detections\n",
    "        results = hands.process(image)\n",
    "        \n",
    "        image.flags.writeable = True\n",
    "        \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # print(results)\n",
    "        \n",
    "        # render the results\n",
    "        if results.multi_hand_landmarks: # if there are landmarks\n",
    "            # looping in each one of the results\n",
    "            for num, hand in enumerate(results.multi_hand_landmarks):\n",
    "                # passing the image, the hand result and the HAND_CONNECTIONS (show the connected relationship)\n",
    "                mp_drawing.draw_landmarks(image, hand, mp_hands.HAND_CONNECTIONS,\n",
    "                                        mp_drawing.DrawingSpec(color=(230, 10 , 128), thickness=2, circle_radius=4),\n",
    "                                        mp_drawing.DrawingSpec(color=(230, 10, 230), thickness=2, circle_radius=2))\n",
    "            \n",
    "        cv2.imshow(\"Hand Tracking\", image)\n",
    "            \n",
    "        if (cv2.waitKey(1) >= 0):\n",
    "            break\n",
    "        \n",
    "cap.release() # Closes video file or capturing device.\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- X = landmark x position in the horizontal axis\n",
    "- y = landmark position in the vertical axis\n",
    "- z = landmark depth from the camera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acessing Landmarks\n",
    "\n",
    "The method bellow can be use to access landmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-6306fe9a7069>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmulti_hand_landmarks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlandmark\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmp_hands\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHandLandmark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPINKY_TIP\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "results.multi_hand_landmarks[0].landmark[mp_hands.HandLandmark.PINKY_TIP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
