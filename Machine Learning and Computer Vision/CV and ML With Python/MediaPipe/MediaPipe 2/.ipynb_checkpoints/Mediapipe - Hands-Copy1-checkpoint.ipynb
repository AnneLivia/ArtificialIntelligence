{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of mediapipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Media Pipe is a framework for building multimodal (e.g video, audio or any time series data),cross-platform (i.e Android, IOS, web, edge devices) applied ML pipelines. Mediapipe also facilitates the deployment of machine learning technology into demos and applications on a wide variety of different hardware platforms.\n",
    "Site information: https://towardsdatascience.com/write-a-few-lines-of-code-and-detect-faces-draw-landmarks-from-complex-images-mediapipe-932f07566d11#:~:text=Media%20Pipe%20is%20a%20framework,variety%20of%20different%20hardware%20platforms.\n",
    "\n",
    "MediaPipe offers open source cross-platform, customizable ML solutions for live and streaming media.\n",
    "Site: https://mediapipe.dev/\n",
    "\n",
    "### Solutions:\n",
    "\n",
    "- Face Detection\n",
    "- Face Mesh\n",
    "- Iris\n",
    "- Pose\n",
    "- Holistic\n",
    "- Selfie Segmentation\n",
    "- Multi-hand Tracking\n",
    "- Hair Segmentation\n",
    "- Instant Motion Tracking\n",
    "- Box Tracking\n",
    "- Objectron\n",
    "- Object Detection and Tracking\n",
    "- Objection: 3D Object Detection and Tracking\n",
    "- AutoFlip: Automatic video cropping pipeline\n",
    "\n",
    "and so on..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is ML pipeline\n",
    "\n",
    "A machine learning pipeline is a way to codify and automate the workflow it takes to produce a machine learning model. Machine learning pipelines consist of multiple sequential steps that do everything from data extraction and preprocessing to model training and deployment.\n",
    "\n",
    "For data science teams, the production pipeline should be the central product. It encapsulates all the learned best practices of producing a machine learning model for the organization’s use-case and allows the team to execute at scale. Whether you are maintaining multiple models in production or supporting a single model that needs to be updated frequently, an end-to-end machine learning pipeline is a must.\n",
    "\n",
    "Site information: https://valohai.com/machine-learning-pipeline/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Pipeline of the MediaPipe Hands: \n",
    "\n",
    "MediaPipe Hands is a high-fidelity hand and finger tracking solution. It employs machine learning (ML) to infer **21 3D landmarks** of a hand from just a single frame. Whereas current state-of-the-art approaches rely primarily on powerful desktop environments for inference, our method achieves real-time performance on a mobile phone, and even scales to multiple hands.\n",
    "\n",
    "*Tracked 3D hand landmarks are represented by dots in different shades, with the brighter ones denoting landmarks closer to the camera.*\n",
    "\n",
    "MediaPipe Hands utilizes an **ML pipeline** consisting of multiple models working together: A **palm detection model** that operates on the full image and returns an **oriented hand bounding box**. A **hand landmark model** that operates on the cropped image region defined by the palm detector and returns **high-fidelity 3D hand keypoints**. This strategy is similar to that employed in our MediaPipe Face Mesh solution, which uses a face detector together with a face landmark model.\n",
    "\n",
    "Providing the accurately cropped hand image to the hand landmark model drastically reduces the need for data augmentation (e.g. rotations, translation and scale) and instead allows the network to dedicate most of its capacity towards coordinate prediction accuracy. In addition, in our pipeline the crops can also be generated based on the hand landmarks identified in the previous frame, and only when the landmark model could no longer identify hand presence is palm detection invoked to relocalize the hand.\n",
    "\n",
    "### Hand Landmark Model:\n",
    "\n",
    "\n",
    "<img src=https://google.github.io/mediapipe/images/mobile/hand_landmarks.png />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and import dependencies:\n",
    "\n",
    "pip is the standard package manager for Python. \n",
    "It allows you to install and manage additional packages that are not part of the Python \n",
    "\n",
    "For installing multiple packages on the command line, just pass them as a space-delimited list, e.g.:\n",
    "pip install mediapipe opencv-python\n",
    "\n",
    "Obs: opencv is already installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in c:\\users\\annel\\anaconda3\\lib\\site-packages (0.8.3)\n",
      "Requirement already satisfied: numpy==1.19.3 in c:\\users\\annel\\anaconda3\\lib\\site-packages (from mediapipe) (1.19.3)\n",
      "Requirement already satisfied: absl-py in c:\\users\\annel\\anaconda3\\lib\\site-packages (from mediapipe) (0.15.0)\n",
      "Requirement already satisfied: dataclasses in c:\\users\\annel\\anaconda3\\lib\\site-packages (from mediapipe) (0.8)\n",
      "Requirement already satisfied: six in c:\\users\\annel\\anaconda3\\lib\\site-packages (from mediapipe) (1.12.0)\n",
      "Requirement already satisfied: wheel in c:\\users\\annel\\anaconda3\\lib\\site-packages (from mediapipe) (0.33.1)\n",
      "Requirement already satisfied: protobuf>=3.11.4 in c:\\users\\annel\\anaconda3\\lib\\site-packages (from mediapipe) (3.19.1)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\annel\\anaconda3\\lib\\site-packages (from mediapipe) (4.5.4.58)\n",
      "Requirement already satisfied: attrs in c:\\users\\annel\\anaconda3\\lib\\site-packages (from mediapipe) (19.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UUID -> Universal Unique Identifier, is a python library which helps in generating random objects of 128 bits as ids. It provides the uniqueness as it generates ids on the basis of time, Computer hardware (MAC etc.). \n",
    "\n",
    "OS -> The OS module in Python provides functions for interacting with the operating system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp # is going to import mediapipe solutions\n",
    "import cv2 # opencv\n",
    "import numpy as np\n",
    "# those libraries bellow are mainly for output process\n",
    "import uuid \n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mp.solutions.drawing_utils is a submodule that is going to be used to draw the detected hand over the image.\n",
    "The drawing_utils sub-module exposes a function that allows to draw the detected face, hand, etc. landmarks over the image\n",
    "landmarks is points on the diagram map to the landmark labels shown, example according to the image above: 0 is the wrist, 20 is the pinky_tip, and so on.\n",
    "\n",
    "landmark detection is a computer vision task where we want to detect and track keypoints from a specific target such as hand or face. This task applies to many problems. For example, we can use the keypoints for detecting a human's head pose position and rotation. With that, we can track whether a driver is paying attention or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mp.solutions.hands is going to load all functionality to perform hand detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now open the webcam using opencv\n",
    "\n",
    "Supported configuration options:\n",
    "\n",
    "- static_image_mode\n",
    "- max_num_hands\n",
    "- model_complexity\n",
    "- min_detection_confidence\n",
    "- min_tracking_confidence\n",
    "\n",
    "\n",
    "STATIC_IMAGE_MODE\n",
    "If set to false, the solution treats the input images as a video stream. It will try to detect hands in the first input images, and upon a successful detection further localizes the hand landmarks. In subsequent images, once all max_num_hands hands are detected and the corresponding hand landmarks are localized, it simply tracks those landmarks without invoking another detection until it loses track of any of the hands. This reduces latency and is ideal for processing video frames. If set to true, hand detection runs on every input image, ideal for processing a batch of static, possibly unrelated, images. Default to false.\n",
    "\n",
    "MAX_NUM_HANDS\n",
    "Maximum number of hands to detect. Default to 2.\n",
    "\n",
    "MODEL_COMPLEXITY\n",
    "Complexity of the hand landmark model: 0 or 1. Landmark accuracy as well as inference latency generally go up with the model complexity. Default to 1.\n",
    "\n",
    "MIN_DETECTION_CONFIDENCE\n",
    "Minimum confidence value ([0.0, 1.0]) from the hand detection model for the detection to be considered successful. Default to 0.5.\n",
    "\n",
    "MIN_TRACKING_CONFIDENCE:\n",
    "Minimum confidence value ([0.0, 1.0]) from the landmark-tracking model for the hand landmarks to be considered tracked successfully, or otherwise hand detection will be invoked automatically on the next input image. Setting it to a higher value can increase robustness of the solution, at the expense of a higher latency. Ignored if static_image_mode is true, where hand detection simply runs on every image. Default to 0.5.\n",
    "\n",
    "Confidence - detection: threshold for the initial detection to be succesful\n",
    "\n",
    "Confidence - tracking: threshold for  tracking after initial detection\n",
    "\n",
    "you can use Python’s **enumerate()** to get a counter and the value from the iterable at the same time!\n",
    "DrawingSpec is a mediapipe class that allows you to customize the look of your detection\n",
    "in this case we are using to customize the landmarks and connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_hands.Hands(min_detection_confidence=0.8, min_tracking_confidence=0.5) as hands:\n",
    "    # start the loop\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        frame = cv2.flip(frame, 1)\n",
    "        \n",
    "        # by default the image color is blue green and red, rgb, red green and blue\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # set flag\n",
    "        image.flags.writeable = False\n",
    "        \n",
    "        # detections\n",
    "        results = hands.process(image)\n",
    "        \n",
    "        image.flags.writeable = True\n",
    "        \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # print(results)\n",
    "        \n",
    "        # render the results\n",
    "        if results.multi_hand_landmarks: # if there are landmarks\n",
    "            # looping in each one of the results\n",
    "            for num, hand in enumerate(results.multi_hand_landmarks):\n",
    "                # passing the image, the hand result and the HAND_CONNECTIONS (show the connected relationship)\n",
    "                mp_drawing.draw_landmarks(image, hand, mp_hands.HAND_CONNECTIONS,\n",
    "                                        mp_drawing.DrawingSpec(color=(230, 10 , 128), thickness=2, circle_radius=4),\n",
    "                                        mp_drawing.DrawingSpec(color=(230, 10, 230), thickness=2, circle_radius=2))\n",
    "            \n",
    "        cv2.imshow(\"Hand Tracking\", image)\n",
    "            \n",
    "        if (cv2.waitKey(1) >= 0):\n",
    "            break\n",
    "        \n",
    "cap.release() # Closes video file or capturing device.\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- X = landmark x position in the horizontal axis\n",
    "- y = landmark position in the vertical axis\n",
    "- z = landmark depth from the camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozenset({(<HandLandmark.WRIST: 0>, <HandLandmark.THUMB_CMC: 1>),\n",
       "           (<HandLandmark.WRIST: 0>, <HandLandmark.INDEX_FINGER_MCP: 5>),\n",
       "           (<HandLandmark.WRIST: 0>, <HandLandmark.PINKY_MCP: 17>),\n",
       "           (<HandLandmark.THUMB_CMC: 1>, <HandLandmark.THUMB_MCP: 2>),\n",
       "           (<HandLandmark.THUMB_MCP: 2>, <HandLandmark.THUMB_IP: 3>),\n",
       "           (<HandLandmark.THUMB_IP: 3>, <HandLandmark.THUMB_TIP: 4>),\n",
       "           (<HandLandmark.INDEX_FINGER_MCP: 5>,\n",
       "            <HandLandmark.INDEX_FINGER_PIP: 6>),\n",
       "           (<HandLandmark.INDEX_FINGER_MCP: 5>,\n",
       "            <HandLandmark.MIDDLE_FINGER_MCP: 9>),\n",
       "           (<HandLandmark.INDEX_FINGER_PIP: 6>,\n",
       "            <HandLandmark.INDEX_FINGER_DIP: 7>),\n",
       "           (<HandLandmark.INDEX_FINGER_DIP: 7>,\n",
       "            <HandLandmark.INDEX_FINGER_TIP: 8>),\n",
       "           (<HandLandmark.MIDDLE_FINGER_MCP: 9>,\n",
       "            <HandLandmark.MIDDLE_FINGER_PIP: 10>),\n",
       "           (<HandLandmark.MIDDLE_FINGER_MCP: 9>,\n",
       "            <HandLandmark.RING_FINGER_MCP: 13>),\n",
       "           (<HandLandmark.MIDDLE_FINGER_PIP: 10>,\n",
       "            <HandLandmark.MIDDLE_FINGER_DIP: 11>),\n",
       "           (<HandLandmark.MIDDLE_FINGER_DIP: 11>,\n",
       "            <HandLandmark.MIDDLE_FINGER_TIP: 12>),\n",
       "           (<HandLandmark.RING_FINGER_MCP: 13>,\n",
       "            <HandLandmark.RING_FINGER_PIP: 14>),\n",
       "           (<HandLandmark.RING_FINGER_MCP: 13>, <HandLandmark.PINKY_MCP: 17>),\n",
       "           (<HandLandmark.RING_FINGER_PIP: 14>,\n",
       "            <HandLandmark.RING_FINGER_DIP: 15>),\n",
       "           (<HandLandmark.RING_FINGER_DIP: 15>,\n",
       "            <HandLandmark.RING_FINGER_TIP: 16>),\n",
       "           (<HandLandmark.PINKY_MCP: 17>, <HandLandmark.PINKY_PIP: 18>),\n",
       "           (<HandLandmark.PINKY_PIP: 18>, <HandLandmark.PINKY_DIP: 19>),\n",
       "           (<HandLandmark.PINKY_DIP: 19>, <HandLandmark.PINKY_TIP: 20>)})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp_hands.HAND_CONNECTIONS\n",
    "# results.multi_hand_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozenset({(<HandLandmark.WRIST: 0>, <HandLandmark.THUMB_CMC: 1>),\n",
       "           (<HandLandmark.WRIST: 0>, <HandLandmark.INDEX_FINGER_MCP: 5>),\n",
       "           (<HandLandmark.WRIST: 0>, <HandLandmark.PINKY_MCP: 17>),\n",
       "           (<HandLandmark.THUMB_CMC: 1>, <HandLandmark.THUMB_MCP: 2>),\n",
       "           (<HandLandmark.THUMB_MCP: 2>, <HandLandmark.THUMB_IP: 3>),\n",
       "           (<HandLandmark.THUMB_IP: 3>, <HandLandmark.THUMB_TIP: 4>),\n",
       "           (<HandLandmark.INDEX_FINGER_MCP: 5>,\n",
       "            <HandLandmark.INDEX_FINGER_PIP: 6>),\n",
       "           (<HandLandmark.INDEX_FINGER_MCP: 5>,\n",
       "            <HandLandmark.MIDDLE_FINGER_MCP: 9>),\n",
       "           (<HandLandmark.INDEX_FINGER_PIP: 6>,\n",
       "            <HandLandmark.INDEX_FINGER_DIP: 7>),\n",
       "           (<HandLandmark.INDEX_FINGER_DIP: 7>,\n",
       "            <HandLandmark.INDEX_FINGER_TIP: 8>),\n",
       "           (<HandLandmark.MIDDLE_FINGER_MCP: 9>,\n",
       "            <HandLandmark.MIDDLE_FINGER_PIP: 10>),\n",
       "           (<HandLandmark.MIDDLE_FINGER_MCP: 9>,\n",
       "            <HandLandmark.RING_FINGER_MCP: 13>),\n",
       "           (<HandLandmark.MIDDLE_FINGER_PIP: 10>,\n",
       "            <HandLandmark.MIDDLE_FINGER_DIP: 11>),\n",
       "           (<HandLandmark.MIDDLE_FINGER_DIP: 11>,\n",
       "            <HandLandmark.MIDDLE_FINGER_TIP: 12>),\n",
       "           (<HandLandmark.RING_FINGER_MCP: 13>,\n",
       "            <HandLandmark.RING_FINGER_PIP: 14>),\n",
       "           (<HandLandmark.RING_FINGER_MCP: 13>, <HandLandmark.PINKY_MCP: 17>),\n",
       "           (<HandLandmark.RING_FINGER_PIP: 14>,\n",
       "            <HandLandmark.RING_FINGER_DIP: 15>),\n",
       "           (<HandLandmark.RING_FINGER_DIP: 15>,\n",
       "            <HandLandmark.RING_FINGER_TIP: 16>),\n",
       "           (<HandLandmark.PINKY_MCP: 17>, <HandLandmark.PINKY_PIP: 18>),\n",
       "           (<HandLandmark.PINKY_PIP: 18>, <HandLandmark.PINKY_DIP: 19>),\n",
       "           (<HandLandmark.PINKY_DIP: 19>, <HandLandmark.PINKY_TIP: 20>)})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"output images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_hands.Hands(min_detection_confidence=0.8, min_tracking_confidence=0.5) as hands:\n",
    "    # start the loop\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        frame = cv2.flip(frame, 1)\n",
    "        \n",
    "        # by default the image color is blue green and red, rgb, red green and blue\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # set flag\n",
    "        image.flags.writeable = False\n",
    "        \n",
    "        # detections\n",
    "        results = hands.process(image)\n",
    "        \n",
    "        image.flags.writeable = True\n",
    "        \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # print(results)\n",
    "        \n",
    "        # render the results\n",
    "        if results.multi_hand_landmarks: # if there are landmarks\n",
    "            # looping in each one of the results\n",
    "            for num, hand in enumerate(results.multi_hand_landmarks):\n",
    "                # passing the image, the hand result and the HAND_CONNECTIONS (show the connected relationship)\n",
    "                mp_drawing.draw_landmarks(image, hand, mp_hands.HAND_CONNECTIONS,\n",
    "                                        mp_drawing.DrawingSpec(color=(230, 10 , 128), thickness=2, circle_radius=4),\n",
    "                                        mp_drawing.DrawingSpec(color=(230, 10, 230), thickness=2, circle_radius=2))\n",
    "        \n",
    "            cv2.imwrite(\n",
    "            os.path.join(\"output images\", \n",
    "            '{}.jpg'.format(uuid.uuid1())), image)\n",
    "        \n",
    "        cv2.imshow(\"Hand Tracking\", image)\n",
    "            \n",
    "        if (cv2.waitKey(1) >= 0):\n",
    "            break\n",
    "        \n",
    "cap.release() # Closes video file or capturing device.\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
