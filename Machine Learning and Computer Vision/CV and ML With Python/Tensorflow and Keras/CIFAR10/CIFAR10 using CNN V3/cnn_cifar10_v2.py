# -*- coding: utf-8 -*-
"""CNN_CIFAR10_V2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15E_zSY4rNsLbAwsKZMDoqPXahZz2q-x1
"""

# This is a dataset of 50,000 32x32 color training images and 10,000 test images, labeled over 10 categories
from keras.datasets import cifar10
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# This is a dataset of 50,000 32x32 color training images and 10,000 test images, labeled over 10 categories
from keras.datasets import cifar10
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

(x_train, y_train), (x_test, y_test) = cifar10.load_data()

x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.5, stratify=y_train)

print(len(y_train), len(y_val), len(y_test))

plt.imshow(x_train[0])
plt.show()

plt.subplot(221)
plt.imshow(x_train[0])
plt.subplot(222)
plt.imshow(x_train[1])
plt.subplot(223)
plt.imshow(x_train[2])
plt.subplot(224)
plt.imshow(x_train[3])
plt.show()

# need to specify 32x32 and 3 channels since it's a colored dataset
x_train = x_train.reshape(x_train.shape[0], 32, 32, 3).astype('float32')
x_test = x_test.reshape(x_test.shape[0], 32, 32, 3).astype('float32')
x_val = x_val.reshape(x_val.shape[0], 32, 32, 3).astype('float32')

# need to normalize values 
x_train = x_train / 255
x_test = x_test / 255
x_val = x_val / 255

import numpy
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.layers import Flatten
from keras.layers.convolutional import Conv2D
from keras.layers.convolutional import MaxPooling2D
from keras.utils import np_utils

y_train = np_utils.to_categorical(y_train)
y_val = np_utils.to_categorical(y_val)
y_test = np_utils.to_categorical(y_test)

num_classes = y_test.shape[1]

print(num_classes)

def baseline_model():
  model = Sequential()
  model.add(Conv2D(90, (3, 3), input_shape=(32, 32, 3), activation='relu'))
  model.add(MaxPooling2D(pool_size = (2, 2)))
  model.add(Dropout(0.2))
  model.add(Conv2D(90, (3, 3), activation='relu'))
  model.add(MaxPooling2D(pool_size = (2, 2)))
  model.add(Dropout(0.3))
  model.add(Conv2D(140, (3, 3), activation='relu'))
  model.add(MaxPooling2D(pool_size = (2, 2)))
  model.add(Dropout(0.3))
  model.add(Flatten())
  model.add(Dense(700, activation='relu'))
  model.add(Dropout(0.3))
  model.add(Dense(540, activation='relu'))
  model.add(Dropout(0.5))
  model.add(Dense(num_classes, activation='softmax'))
  model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
  return model

model = baseline_model()

model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=250, batch_size=260)

scores = model.evaluate(x_test, y_test)
result_error = str('%.2f'%(1 - scores[1]))
print('Error: ', result_error)

!pip install pyyaml h5py

from google.colab import drive
drive.mount('/content/gdrive')

model_json = model.to_json()
with open("/content/gdrive/My Drive/Colab Notebooks/model_Livia.json" , "w") as json_file:
    json_file.write(model_json)

#serialize weights to HDF5
model.save_weights("/content/gdrive/My Drive/Colab Notebooks/model_w_Livia.h5" )