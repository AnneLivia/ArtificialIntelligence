#include <iostream>
#include <vector>
#include <cmath>
#include <sstream>
#include <fstream>
#include <opencv2/opencv.hpp>
#include <algorithm>

using namespace std;
using namespace cv;

/*
    reference: Machine learning mastery
    Gradient decent is a way to calculate linear regression.

    Gradient descent is an optimization algorithm used to find the
    values of parameters (coefficients) of a function (f) that
    minimizes a cost function (cost).

    Gradient descent is best used when the parameters cannot be calculated analytically
    (e.g. using linear algebra) and must be searched for by an optimization algorithm.

    Gradient descent is important because sometimes we're gonna have a dataset with many
    variables, and a close formula is not going to solve it always.

    It does many smalls tries to check if the coefficients are the best fit

    gradient descent does a lot of guess to find the best fit

    error (residual) = y (real value) - guess

    the idea is to minimize the residual.

    we use the error to change the vector in the gradient descent

    this is supervised learning.

    we can start with random values for m and b and it repeats until it gets a good answer or
    the number of iterations finishes (normally m and b starts with 0).

    LOSS Function:
        the loss function/cost function is used to minimize the error.
        it evaluates the performance of the machine learning

        The Loss function computes the error for a single training
        example while the Cost function is the average of the loss
        functions for all the training examples

        A Cost function basically tells how good is the model
        at making predictions for a given value of m and b.

        The are many ways to calculate the error, in this program it's being used the
        squared error

    Hyperparemeter:
        A model hyperparameter is a configuration that is external to
        the model and whose value cannot be estimated from data.
        They are often specified by the practitioner.
        They can often be set using heuristics.
        They are often tuned for a given predictive modeling problem.
        In this program it's being used the learning_rate that defines how fast
        the model learns.
        The number of the learning rate can neither be small neither big,
        if it's too low, the model will be too slow to converge (find optimal value)
        if it's too high, the model will never converge.
        Everything in machine learning must be balanced.
        In general in machine learning we don't know what exactly those value are going to be
        so we have to guess and check.
    Gradient:
        find the point where the error is smallest.
        find the minimum local. the graph generated by all possible movements forms a ball curve
        The computation of this is done by calculating the gradient known as slope (derivation Calculus)
        in every iteration we want to move the point where we are to the smallest point.
        gradient have a direction, is a vector.

        if we can compute the gradient of a data that means that the function is differentiable
        the tangent get from the partial derivation gives the direction to move the point

        to calculate must get the partial derivative with respect of m and b

        When we have a linear function we always are going to have one local minimum.
        for most complexity data, we maybe have more than one. several local minimum.

        if a function is differentiable then we can optimize it
*/

struct Variable {
    // can increase number of variables here
    double x, y;

    bool operator < (const Variable& v2) {
        return this->x < v2.x;
    }
};

class LinearRegression {
private:
    vector<Variable> data;
    int n, num_iterations;
    double m, b; // y = m*x + b
    double learning_rate; // hyperparameter, defines how fast model learns

public:
    LinearRegression(vector<Variable> data) : data(data) {
        this->n = (int)data.size();
        // starts with 0, they're going to be learn over time
        this->m = 0.0; // random value
        this->b = 0.0; // random value
        this->learning_rate = 0.0001;
        // defines how many iteration the train step is going to run
        // as the number of data grows, the number of iterations must grow too
        this->num_iterations = 1000;
    }

    double cost_function() {
        /*
            this function returns the error value, it's a way to estimate how bad the line is
            so that in every step it improves the function.
            In this case we want to measure the distance from each point to the line.
            Square Error (it's squared because we don't care about the negative part of it)
                            n
            Formula: 1/n * Sum   (Real Y - (m*xi + b))^2
                           i = 1

            So is the sum of the residuals / total number of elements
        */
        double sum = 0.0;
        for(int i = 0; i < this->n; i++) {
            sum+=pow(this->data[i].y - (this->m * this->data[i].x + this->b), 2);
        }

        return sum / this->n; // return the sum of the squared errors (error value)
    }

    void step_gradient() {
        /* all of the gradient descent runs here
            must find direction to move the point in each iteration using partial derivative
            formula:
                d      2      n
               --- =  ---  * Sum  ( - x * (yi - (m*xi + b))) --> gives M
               d m     N    i = 1

                d      2      n
               --- =  ---  * Sum  (- (yi - (m*xi + b))) --> gives B
               d b     N    i = 1
        */

        double b_grad = 0.0;
        double m_grad = 0.0;
        for (int i = 0; i < this->n; i++) {
            // must concatenate the value of all the points
            m_grad+= -(2.0/this->n) * (this->data[i].x * (this->data[i].y - (this->m * this->data[i].x + this->b)));
            b_grad+= -(2.0/this->n) * (this->data[i].y - (this->m * this->data[i].x + this->b));
        }

        // gradient tells which way and learning rate tells how far to move
        this->m = this->m - (learning_rate * m_grad);
        this->b = this->b - (learning_rate * b_grad);
    }

    // this function is going to return the optimal values for m and b
    pair<double, double> gradientDescent() {
        // this will run num-iterations times until it finds the best values
        for (int i = 0; i < this->num_iterations; i++) {
            // must return new value according to the learning rate
            step_gradient();
        }

        return {this->m, this->b};
    }
};

int main()
{
    vector<Variable> data; // = {{43, 99}, {21, 65}, {25, 79}, {42, 75}, {57, 87}, {59, 81}};

    // reading data
    ifstream dataset("data.csv");

    string datastr, value;

    while(dataset >> datastr) {
        stringstream str(datastr);
        // to store first value in x and second in y
        int cont = 1;
        double x, y;
        while(getline(str, value, ',')) {
            if (cont == 1) {
                x = stod(value);
            } else {
                y = stod(value);
            }
            cont++;
        }

        data.push_back({x,y});
    }

    dataset.close();

    LinearRegression lr(data);

    pair<double, double> coeff = lr.gradientDescent();

    Mat graph(Size(400, 400), CV_64FC3, Scalar(255,255,255));
    // y-axis
    line(graph, Point(200, 0), Point(200, 500), Scalar(255, 0, 0), 2);
    // x-axis
    line(graph, Point(0, 200), Point(500, 200), Scalar(255, 0, 0), 2);

    // plotting points
    for (int i = 0; i < (int)data.size(); i++) {
        double x = data[i].x;
        double y = data[i].y;
        line(graph, Point(x + 200, 200 - y), Point(x + 200, 200 - y), Scalar(0, 255, 0), 4);
    }

    cout << "Y = " << coeff.first << "X + " << coeff.second << endl;
    cout << "Cost Function: " << lr.cost_function() << endl;

    // draw fit line
    // in order to get max and min value of the data
    sort(data.begin(), data.end());

    line(graph, Point(data[0].x + 200, 200 - (coeff.first * data[0].x + coeff.second)),
                Point(data[(int)data.size() - 1].x + 200, 200 - (coeff.first * data[(int)data.size() - 1].x + coeff.second)),
        Scalar(255, 0, 0), 2);

    namedWindow("Graph", WINDOW_NORMAL);
    imshow("Graph", graph);

    waitKey();


    return 0;
}
